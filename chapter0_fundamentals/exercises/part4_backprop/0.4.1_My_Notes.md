# Step-by-step summary of creating autograd
1. log_back(grad_out, out, x)
x --(log)--> out --~~--> L
log_back := dL/dx
grad_out := dL/d(out)

2. unbroadcast
x -> xb
unbroadcast does the reverse: shrink #dims -> summation along dims that were expanded

3. multiply_back0 and 1 (grad_out, out, x, y)
x -\
    --(mult)--> out --~~--> L
y -/

grad_out := dL/d(out)
multiply_back0 := dL/dx
need to use unbroadcast coz x * y = x_b * y_b, but we want dL/dx, while grad_out is dL/d(xb * yb)

4. quick test for a computation graph that has mult and log, tracking forward and backward (at leaves)

5. recipe: func, args, kwargs, parents

6. backward function lookup: for each (func, arg_pos) there's an associated backward function like log_back or multiply_back1

7. Tensor wraper: array, requires_grad, is_leaf, recipe

8. forwards pass: log_forward, multiply_forward

9. wrap_forward_fn: np_func, is_differentiable -> tensor_func