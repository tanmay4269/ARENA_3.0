> I am taking these notes to be able to reproduce this autograd in cpp from scratch
# Everything I did in this assignment
1. Implementing: 
    1. `log_back(grad_out: Arr, out: Arr, x: Arr) -> Arr`
    2. `unbroadcast(new: Arr, old: Arr) -> Arr`
    3. `multiply_back0(grad_out: Arr, out: Arr, x: Arr, y: Arr | float) -> Arr` and multiply_back1
    - This will later (see point 4 below) be useful as a repository of backward functions
2. Testing if forward and backward work fine for a simple enough computational graph
    - I could be more regrous with this kind of validation at multiple stages 
3. Recipe: func, args, kwargs, parents
4. BACK_FUNCS = {key: position, value: back_fn}
    - add in the three functions implemented so far
5. Tensor: array: Arr, requires_grad: bool, grad: Tensor, recipe: Recipe
6. Implementing:
    1. `log_forward(x: Tensor) -> Tensor`
    2. `multiply_forward(a: Tensor | int, b: Tensor | int) -> Tensor`
    - They warp the arrays in a tensor, filling the 4 things mentioned above
7. `wrap_forward_fn(numpy_func: Callable, is_differentiable=True) -> Callable`
    - Makes any np function do what a tensor needs (the 4 things) to have after the foward pass on that node
    - All the forward counterpards of the functions below are passed into this function which the Tensor class calls when operating over tensors
8. Implementing actual backprop: 
    `backprop(end_node: Tensor, end_grad: Tensor | None = None) -> None`
9. Implementing more backward functions for MNIST training
    1. `negative_back(grad_out: Arr, out: Arr, x: Arr) -> Arr`
        - used in unary negation in Tensor
    2. `exp_back(grad_out: Arr, out: Arr, x: Arr) -> Arr`
        - used in cross entropy
    3. `reshape_back(grad_out: Arr, out: Arr, x: Arr, new_shape: tuple) -> Arr`
        - [!] not used where grads were needed
    4. `permute_back(grad_out: Arr, out: Arr, x: Arr, axes: tuple) -> Arr`
        - used for transposing 
    5. `sum_back(grad_out: Arr, out: Arr, x: Arr, dim=None, keepdim=False)`
        - used in cross entropy
    6. np.add, np.subtract, np.true_divide's back_fns
        - used widely
    7. indexing: `getitem_back(grad_out: Arr, out: Arr, x: Arr, index: Index)`
        - used in cross entropy
    8. argmax's forward: `_argmax(x: Arr, dim=None, keepdim=False)`
        - used to get predictions from model logits (ie. output)
    9. `maximum_back0(grad_out: Arr, out: Arr, x: Arr, y: Arr)` and maximum_back1
        - Used in relu
    10. `matmul2d_back0(grad_out: Arr, out: Arr, x: Arr, y: Arr) -> Arr` and matmul2d_back1
        - Tensor.__matmul__() <=> x @ y
    - [!] Some are actually not needed for simple MNIST training... 
10. Abstraction++
    1. Parameter: Tensor
    2. Module: modules, params, setters, call->forward
    3. Linear: weight, bias, forward()
        - Needs matmul properly made (forward and backward)
    4. ReLU
    5. MLP: linear & relu only
11. Last few things for training:
    1. `cross_entropy(logits: Tensor, true_labels: Tensor) -> Tensor`
    2. NoGrad context manager
    3. SGD: parameters, zero_grad(), step()
    4. Simple Dataloader
12. Finally, just train and test it
