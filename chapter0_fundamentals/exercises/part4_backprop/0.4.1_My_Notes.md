> I am taking these notes to be able to reproduce this autograd in cpp from scratch
# Everything I did in this assignment
1. Implementing: 
    1. `log_back(grad_out: Arr, out: Arr, x: Arr) -> Arr`
    2. `unbroadcast(new: Arr, old: Arr) -> Arr`
    3. `multiply_back0(grad_out: Arr, out: Arr, x: Arr, y: Arr | float) -> Arr` and multiply_back1
    - This will later (see point 4 below) be useful as a repository of backward functions
2. Testing if forward and backward work fine for a simple enough computational graph
    - I could be more regrous with this kind of validation at multiple stages 
3. Recipe: func, args, kwargs, parents
4. BACK_FUNCS = {key: position, value: back_fn}
    - add in the three functions implemented so far
5. Tensor: array: Arr, requires_grad: bool, grad: Tensor, recipe: Recipe
6. Implementing:
    1. `log_forward(x: Tensor) -> Tensor`
    2. `multiply_forward(a: Tensor | int, b: Tensor | int) -> Tensor`
    - They warp the arrays in a tensor, filling the 4 things mentioned above
7. `wrap_forward_fn(numpy_func: Callable, is_differentiable=True) -> Callable`
    - Makes any np function do what a tensor needs (the 4 things) to have after the foward pass on that node
8. Implementing actual backprop: 
    `backprop(end_node: Tensor, end_grad: Tensor | None = None) -> None`
9. Implementing more backward functions for MNIST training
    1. `negative_back(grad_out: Arr, out: Arr, x: Arr) -> Arr`
    2. `exp_back(grad_out: Arr, out: Arr, x: Arr) -> Arr`
    3. `reshape_back(grad_out: Arr, out: Arr, x: Arr, new_shape: tuple) -> Arr`
    4. `permute_back(grad_out: Arr, out: Arr, x: Arr, axes: tuple) -> Arr`
    5. `sum_back(grad_out: Arr, out: Arr, x: Arr, dim=None, keepdim=False)`
    6. np.add, np.subtract, np.true_divide's back_fns
    7. indexing: `getitem_back(grad_out: Arr, out: Arr, x: Arr, index: Index)`
    8. argmax's forward: `_argmax(x: Arr, dim=None, keepdim=False)`
    9. `maximum_back0(grad_out: Arr, out: Arr, x: Arr, y: Arr)` and maximum_back1
    10. `matmul2d_back0(grad_out: Arr, out: Arr, x: Arr, y: Arr) -> Arr` and matmul2d_back1
    - [!] Some are actually not needed for simple MNIST training... 
10. Abstraction++
    1. Parameter
    2. Module
    3. Linear
    4. MLP
11. Last few things for training:
    1. `cross_entropy(logits: Tensor, true_labels: Tensor) -> Tensor`
    2. NoGrad context manager
    3. SGD
    4. Simple Dataloader
12. Finally, just train and test it
